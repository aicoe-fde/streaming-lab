{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using the [markovify](https://github.com/jsvine/markovify/) library to make some social-media-sized utterances in the style of Jane Austen.  This will be the basis for generating a synthetic social media stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import codecs\n",
    "\n",
    "with codecs.open(\"austen.txt\", \"r\", \"cp1252\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "austen_model = markovify.Text(text, retain_original=False, state_size=3)\n",
    "\n",
    "for i in range(10):\n",
    "    print(austen_model.make_short_sentence(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use spaCy to identify entities (mostly proper nouns and noun phrases) in these synthetic status updates and turn them into hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def make_sentence(model, length=200):\n",
    "    return model.make_short_sentence(length)\n",
    "    \n",
    "def hashtagify_full(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        sentence = sentence.replace(str(ent), \"#%s\" % str(ent).replace(\" \", \"\"))\n",
    "    return (sentence, [\"#%s\" % str(ent).replace(\" \", \"\") for ent in doc.ents])\n",
    "\n",
    "def hashtagify(sentence):\n",
    "    result,_ = hashtagify_full(sentence)\n",
    "    return result\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = make_sentence(austen_model)\n",
    "    print(sentence)\n",
    "    print(hashtagify(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train two Markov models on positive and negative product reviews (taken from the [public-domain Amazon fine foods reviews dataset on Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews/)).  We'll incorporate the results of these models into our synthetic social media stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def train_markov_gz(fn):\n",
    "    \"\"\" trains a Markov model on gzipped text data \"\"\"\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return markovify.Text(text, retain_original=False, state_size=3)\n",
    "\n",
    "negative_model = train_markov_gz(\"reviews-1.txt.gz\")\n",
    "positive_model = train_markov_gz(\"reviews-5-100k.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sentence(negative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sentence(positive_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these models with relative weights, but this yields somewhat unusual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model = markovify.combine([austen_model, negative_model, positive_model], [14, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(hashtagify(make_sentence(compound_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is more or less the case in the real world, we'll assume that a small percentage of users are responsible for the bulk of social media activity, and that the bulk of users are responsible for relatively few posts.  We'll model this with a table of random user IDs that has a collection of relatively few talkative users and relatively many moderate users; the proportion of utterances from talkative users to utterances from moderate users is the inverse of the proportion of talkative users to moderate users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "import collections\n",
    "\n",
    "class UserTable(object):\n",
    "    def __init__(self, size, weights=[8, 2]):\n",
    "        self._talkative = collections.deque()\n",
    "        self._moderate = collections.deque()\n",
    "        self._size = size\n",
    "        self._cutoff = float(weights[0]) / sum(weights)\n",
    "        \n",
    "        for i in range(size):\n",
    "            new_uid = math.floor(numpy.random.uniform(10 ** 10))\n",
    "            if numpy.random.uniform() >= self._cutoff:\n",
    "                self._moderate.append(new_uid)\n",
    "            else:\n",
    "                self._talkative.append(new_uid)\n",
    "    \n",
    "    def random_uid(self):\n",
    "        def choose_from(c):\n",
    "            return c[math.floor(numpy.random.uniform() * len(c))]\n",
    "        \n",
    "        if numpy.random.uniform() >= self._cutoff:\n",
    "            return choose_from(self._talkative)\n",
    "        else:\n",
    "            return choose_from(self._moderate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of times each user ID appears if we ask the `UserTable` for 1000 random user IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut = UserTable(100)\n",
    "uids = [ut.random_uid() for i in range(1000)]\n",
    "seaborn.countplot(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweets(models, weights=None, hashtag_weights=[8, 2], ut=None, seed_hashtags=[]):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models)\n",
    "    \n",
    "    if ut is None:\n",
    "        ut = UserTable(10000)\n",
    "    \n",
    "    choices = []\n",
    "    \n",
    "    total_weight = float(sum(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        choices.append((float(sum(weights[0:i+1])) / total_weight, models[i]))\n",
    "    \n",
    "    def choose_model():\n",
    "        r = numpy.random.uniform()\n",
    "        for (p, m) in choices:\n",
    "            if r <= p:\n",
    "                return m\n",
    "        return choices[-1][1]\n",
    "    \n",
    "    seen_hashtags = set()\n",
    "    hashtags = []\n",
    "    total_hashtag_weight = float(sum(hashtag_weights))\n",
    "    for i in range(len(hashtag_weights)): \n",
    "        hashtags.append((float(sum(hashtag_weights[0:i+1])) / total_hashtag_weight, collections.deque()))\n",
    "    \n",
    "    def choose_from(c):\n",
    "        idx = math.floor(numpy.random.uniform() * len(c))\n",
    "        return c[idx]\n",
    "    \n",
    "    def store_hashtag(tag):\n",
    "        if tag not in seen_hashtags:\n",
    "            seen_hashtags.add(tag)\n",
    "            r = numpy.random.uniform()\n",
    "            for(p, deq) in hashtags:\n",
    "                if r <= p:\n",
    "                    deq.append(tag)\n",
    "    \n",
    "    def choose_hashtag():\n",
    "        r = numpy.random.uniform()\n",
    "        for(p, deq) in hashtags:\n",
    "            if r <= 1.0 - p:\n",
    "                return choose_from(deq)\n",
    "        return choose_from(hashtags[0][1])\n",
    "    \n",
    "    for tag in seed_hashtags:\n",
    "        store_hashtag(str(tag))\n",
    "    \n",
    "    while True:\n",
    "        tweet, tags = hashtagify_full(make_sentence(choose_model()))\n",
    "        for tag in tags:\n",
    "            store_hashtag(str(tag))\n",
    "        \n",
    "        this_tweet_tags = set([str(t) for t in tags])\n",
    "        \n",
    "        if len(seen_hashtags) > 0:\n",
    "            for i in range(min(numpy.random.poisson(3), len(seen_hashtags))):\n",
    "                tag = choose_hashtag()\n",
    "                if str(tag) not in this_tweet_tags:\n",
    "                    this_tweet_tags.add(str(tag))\n",
    "                    tweet += \" %s\" % str(tag)\n",
    "            \n",
    "        yield (ut.random_uid(), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_hashtags=[\"#ff\", \"#marketing\", \"#fail\", \"#followfriday\", \"#yolo\", \"#retweet\", \"#tbt\", \"#socialmedia\", \"#startup\", \"#blogpost\", \"#news\", \"#health\"]\n",
    "\n",
    "t = generate_tweets([austen_model, positive_model, negative_model, compound_model], [22, 4, 4, 2], seed_hashtags=seed_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[next(t) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def timing(c):\n",
    "    for _ in range(c):\n",
    "        next(t)\n",
    "\n",
    "cProfile.run('timing(2000)', 'generatestats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "p = pstats.Stats('generatestats')\n",
    "p.strip_dirs().sort_stats(-1).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
